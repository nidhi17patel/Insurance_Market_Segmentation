{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d12fcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "def38971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset again\n",
    "df = pd.read_csv(\"Clustered_Customer_Data.csv\")\n",
    "\n",
    "# Drop unnecessary column\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[\"Cluster\"])\n",
    "y = df[\"Cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6663c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Imbalanced Data\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Train-test split (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba828c",
   "metadata": {},
   "source": [
    "# Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0f8fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:02:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Decision Tree Results\n",
      "Accuracy: 0.9619736015084852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       796\n",
      "           1       0.98      0.99      0.99       795\n",
      "           2       0.96      0.96      0.96       796\n",
      "           3       0.95      0.94      0.95       795\n",
      "\n",
      "    accuracy                           0.96      3182\n",
      "   macro avg       0.96      0.96      0.96      3182\n",
      "weighted avg       0.96      0.96      0.96      3182\n",
      "\n",
      "Confusion Matrix:\n",
      " [[760   0  20  16]\n",
      " [  0 786   1   8]\n",
      " [ 18   1 765  12]\n",
      " [ 25  11   9 750]]\n",
      "\n",
      "üìå Random Forest Results\n",
      "Accuracy: 0.9798868636077939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       796\n",
      "           1       0.98      1.00      0.99       795\n",
      "           2       0.97      1.00      0.99       796\n",
      "           3       0.98      0.96      0.97       795\n",
      "\n",
      "    accuracy                           0.98      3182\n",
      "   macro avg       0.98      0.98      0.98      3182\n",
      "weighted avg       0.98      0.98      0.98      3182\n",
      "\n",
      "Confusion Matrix:\n",
      " [[765   1  17  13]\n",
      " [  0 795   0   0]\n",
      " [  1   1 794   0]\n",
      " [ 13  14   4 764]]\n",
      "\n",
      "üìå XGBoost Results\n",
      "Accuracy: 0.9849151477058454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       796\n",
      "           1       0.99      1.00      0.99       795\n",
      "           2       0.98      0.99      0.99       796\n",
      "           3       0.99      0.96      0.98       795\n",
      "\n",
      "    accuracy                           0.98      3182\n",
      "   macro avg       0.98      0.98      0.98      3182\n",
      "weighted avg       0.98      0.98      0.98      3182\n",
      "\n",
      "Confusion Matrix:\n",
      " [[780   0  12   4]\n",
      " [  0 795   0   0]\n",
      " [  2   0 792   2]\n",
      " [ 13  10   5 767]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "\n",
    "# Train models\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"üìå Decision Tree Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, dt_preds))\n",
    "print(classification_report(y_test, dt_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, dt_preds))\n",
    "\n",
    "print(\"\\nüìå Random Forest Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(classification_report(y_test, rf_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "print(\"\\nüìå XGBoost Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds))\n",
    "print(classification_report(y_test, xgb_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71f2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Tuning Decision Tree...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "üîç Tuning Random Forest...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "üîç Tuning XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:33:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Best Decision Tree Params: {'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 15}\n",
      "Best DT Accuracy: 0.9555241238409554\n",
      "\n",
      "üìå Best Random Forest Params: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 20}\n",
      "Best RF Accuracy: 0.9754046833254755\n",
      "\n",
      "üìå Best XGBoost Params: {'subsample': 1, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1}\n",
      "Best XGB Accuracy: 0.9849127769919849\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grids\n",
    "dt_params = {\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1],\n",
    "    \"colsample_bytree\": [0.8, 1]\n",
    "}\n",
    "\n",
    "# Run RandomizedSearchCV for each model\n",
    "dt_search = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), dt_params, n_iter=10, cv=3, n_jobs=-1, verbose=1, random_state=42)\n",
    "rf_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_params, n_iter=10, cv=3, n_jobs=-1, verbose=1, random_state=42)\n",
    "xgb_search = RandomizedSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42), xgb_params, n_iter=10, cv=3, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "# Fit the models\n",
    "print(\"üîç Tuning Decision Tree...\")\n",
    "dt_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"üîç Tuning Random Forest...\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"üîç Tuning XGBoost...\")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"\\nüìå Best Decision Tree Params:\", dt_search.best_params_)\n",
    "print(\"Best DT Accuracy:\", dt_search.best_score_)\n",
    "\n",
    "print(\"\\nüìå Best Random Forest Params:\", rf_search.best_params_)\n",
    "print(\"Best RF Accuracy:\", rf_search.best_score_)\n",
    "\n",
    "print(\"\\nüìå Best XGBoost Params:\", xgb_search.best_params_)\n",
    "print(\"Best XGB Accuracy:\", xgb_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882b53f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Tuning Decision Tree with GridSearchCV...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "üîç Tuning Random Forest with GridSearchCV...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "üîç Tuning XGBoost with GridSearchCV...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:16:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Optimized Decision Tree Params: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best DT Accuracy: 0.9603965695254469\n",
      "\n",
      "üìå Optimized Random Forest Params: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 200}\n",
      "Best RF Accuracy: 0.9765050149932788\n",
      "\n",
      "üìå Optimized XGBoost Params: {'colsample_bytree': 1, 'learning_rate': 0.12, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.9}\n",
      "Best XGB Accuracy: 0.9875844847729093\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Updated parameter grids (smaller search space around best params)\n",
    "dt_grid = {\n",
    "    \"max_depth\": [10, 15, 20],\n",
    "    \"min_samples_split\": [2, 3, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [150, 200, 250],\n",
    "    \"max_depth\": [15, 20, 25],\n",
    "    \"min_samples_split\": [4, 5, 6],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "xgb_grid = {\n",
    "    \"n_estimators\": [250, 300, 350],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"learning_rate\": [0.08, 0.1, 0.12],\n",
    "    \"subsample\": [0.9, 1],\n",
    "    \"colsample_bytree\": [0.9, 1]\n",
    "}\n",
    "\n",
    "# Initialize models with best parameters from RandomizedSearchCV\n",
    "dt_model = DecisionTreeClassifier(random_state=42, criterion=\"entropy\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "\n",
    "# GridSearchCV for each model\n",
    "dt_search = GridSearchCV(dt_model, dt_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "rf_search = GridSearchCV(rf_model, rf_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "xgb_search = GridSearchCV(xgb_model, xgb_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the models\n",
    "print(\"üîç Tuning Decision Tree with GridSearchCV...\")\n",
    "dt_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"üîç Tuning Random Forest with GridSearchCV...\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"üîç Tuning XGBoost with GridSearchCV...\")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"\\nüìå Optimized Decision Tree Params:\", dt_search.best_params_)\n",
    "print(\"Best DT Accuracy:\", dt_search.best_score_)\n",
    "\n",
    "print(\"\\nüìå Optimized Random Forest Params:\", rf_search.best_params_)\n",
    "print(\"Best RF Accuracy:\", rf_search.best_score_)\n",
    "\n",
    "print(\"\\nüìå Optimized XGBoost Params:\", xgb_search.best_params_)\n",
    "print(\"Best XGB Accuracy:\", xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c833920",
   "metadata": {},
   "source": [
    "# converting it to model\n",
    "import joblib\n",
    "\n",
    "# Save the best XGBoost model\n",
    "joblib.dump(xgb_search.best_estimator_, \"customer_model.pkl\")\n",
    "\n",
    "print(\"‚úÖ XGBoost Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ddeb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619736015084852 % Acuuracy\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'customer_model.sav'\n",
    "pickle.dump(dt_model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result,'% Acuuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
